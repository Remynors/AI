{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = pd.get_dummies(data, dummy_na=True, drop_first=True)\n",
    "data.drop('Id', axis=1, inplace=True)\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0d8de44658ffce2ab83bb94fcefd9eb3b138b5b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace=True)\n",
    "columns = data.columns\n",
    "sale_price = data['SalePrice']\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8a0b45fba7c875469ebf4751a111beae0f6f53ed",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/norvilr/anaconda3/envs/torch10/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.235294     0.150685  0.033420     0.666667        0.500   0.949275   \n",
       "1    0.000000     0.202055  0.038795     0.555556        0.875   0.753623   \n",
       "2    0.235294     0.160959  0.046507     0.666667        0.500   0.934783   \n",
       "3    0.294118     0.133562  0.038561     0.666667        0.500   0.311594   \n",
       "4    0.235294     0.215753  0.060576     0.777778        0.500   0.927536   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2        ...          \\\n",
       "0      0.883333     0.12250    0.125089         0.0        ...           \n",
       "1      0.433333     0.00000    0.173281         0.0        ...           \n",
       "2      0.866667     0.10125    0.086109         0.0        ...           \n",
       "3      0.333333     0.00000    0.038271         0.0        ...           \n",
       "4      0.833333     0.21875    0.116052         0.0        ...           \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleType_nan  \\\n",
       "0           0.0           0.0          1.0           0.0   \n",
       "1           0.0           0.0          1.0           0.0   \n",
       "2           0.0           0.0          1.0           0.0   \n",
       "3           0.0           0.0          1.0           0.0   \n",
       "4           0.0           0.0          1.0           0.0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                    0.0                   0.0                   0.0   \n",
       "1                    0.0                   0.0                   0.0   \n",
       "2                    0.0                   0.0                   0.0   \n",
       "3                    0.0                   0.0                   0.0   \n",
       "4                    0.0                   0.0                   0.0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  SaleCondition_nan  \n",
       "0                   1.0                    0.0                0.0  \n",
       "1                   1.0                    0.0                0.0  \n",
       "2                   1.0                    0.0                0.0  \n",
       "3                   0.0                    0.0                0.0  \n",
       "4                   1.0                    0.0                0.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['SalePrice'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2919 entries, 0 to 2918\n",
      "Columns: 289 entries, MSSubClass to SaleCondition_nan\n",
      "dtypes: float64(289)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500.0\n",
       "1    181500.0\n",
       "2    223500.0\n",
       "3    140000.0\n",
       "4    250000.0\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SalePrice'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c0a3ac6dc08a788c0e979dcc86ef5889b2a78926"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/norvilr/anaconda3/envs/torch10/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train = data.iloc[:1460]\n",
    "test = data.iloc[1460:]\n",
    "test.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5f579ce99580cb2adc657aaaac39221b4d914a1b"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cc525cb3a31c6fa0e72ba960f0619f15f142bc2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 288)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d5beea42af0aef85dd90c34e07acb5c44a560204"
   },
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(288, 144)\n",
    "        self.fc2 = nn.Linear(144, 72)\n",
    "        self.fc3 = nn.Linear(72, 18)\n",
    "        self.fc4 = nn.Linear(18, 1)\n",
    "\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = self.dropout(F.relu(self.fc1(x)))\n",
    "        #x = self.dropout(F.relu(self.fc2(x)))\n",
    "        #x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "3ed36197761510ee988113190dd0b95701980e31"
   },
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 50)\n",
    "label_batch = np.array_split(y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "5d082c462da5fa8beeddcb6c1c3389ec91388f0b"
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "X_val = torch.from_numpy(X_val.values).float()\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "350d12e01c36c9c21199dcbb58c6518824f9f0c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Regressor()\n",
    "ps = model(train_batch[0])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Regressor(\n",
       "  (fc1): Linear(in_features=288, out_features=144, bias=True)\n",
       "  (fc2): Linear(in_features=144, out_features=72, bias=True)\n",
       "  (fc3): Linear(in_features=72, out_features=18, bias=True)\n",
       "  (fc4): Linear(in_features=18, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1f905b903415928642a3b05e4d05eacc6c825fcb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300..  Training Loss: 11.335..  Test Loss: 9.221.. \n",
      "Epoch: 2/300..  Training Loss: 8.226..  Test Loss: 7.357.. \n",
      "Epoch: 3/300..  Training Loss: 6.814..  Test Loss: 6.266.. \n",
      "Epoch: 4/300..  Training Loss: 5.897..  Test Loss: 5.493.. \n",
      "Epoch: 5/300..  Training Loss: 5.208..  Test Loss: 4.876.. \n",
      "Epoch: 6/300..  Training Loss: 4.646..  Test Loss: 4.368.. \n",
      "Epoch: 7/300..  Training Loss: 4.179..  Test Loss: 3.940.. \n",
      "Epoch: 8/300..  Training Loss: 3.781..  Test Loss: 3.570.. \n",
      "Epoch: 9/300..  Training Loss: 3.432..  Test Loss: 3.243.. \n",
      "Epoch: 10/300..  Training Loss: 3.121..  Test Loss: 2.948.. \n",
      "Epoch: 11/300..  Training Loss: 2.839..  Test Loss: 2.680.. \n",
      "Epoch: 12/300..  Training Loss: 2.582..  Test Loss: 2.434.. \n",
      "Epoch: 13/300..  Training Loss: 2.344..  Test Loss: 2.204.. \n",
      "Epoch: 14/300..  Training Loss: 2.119..  Test Loss: 1.987.. \n",
      "Epoch: 15/300..  Training Loss: 1.907..  Test Loss: 1.783.. \n",
      "Epoch: 16/300..  Training Loss: 1.710..  Test Loss: 1.594.. \n",
      "Epoch: 17/300..  Training Loss: 1.526..  Test Loss: 1.418.. \n",
      "Epoch: 18/300..  Training Loss: 1.354..  Test Loss: 1.252.. \n",
      "Epoch: 19/300..  Training Loss: 1.186..  Test Loss: 1.088.. \n",
      "Epoch: 20/300..  Training Loss: 1.024..  Test Loss: 0.935.. \n",
      "Epoch: 21/300..  Training Loss: 0.874..  Test Loss: 0.797.. \n",
      "Epoch: 22/300..  Training Loss: 0.740..  Test Loss: 0.676.. \n",
      "Epoch: 23/300..  Training Loss: 0.623..  Test Loss: 0.575.. \n",
      "Epoch: 24/300..  Training Loss: 0.525..  Test Loss: 0.496.. \n",
      "Epoch: 25/300..  Training Loss: 0.450..  Test Loss: 0.441.. \n",
      "Epoch: 26/300..  Training Loss: 0.398..  Test Loss: 0.407.. \n",
      "Epoch: 27/300..  Training Loss: 0.366..  Test Loss: 0.389.. \n",
      "Epoch: 28/300..  Training Loss: 0.349..  Test Loss: 0.380.. \n",
      "Epoch: 29/300..  Training Loss: 0.339..  Test Loss: 0.375.. \n",
      "Epoch: 30/300..  Training Loss: 0.333..  Test Loss: 0.372.. \n",
      "Epoch: 31/300..  Training Loss: 0.328..  Test Loss: 0.368.. \n",
      "Epoch: 32/300..  Training Loss: 0.325..  Test Loss: 0.365.. \n",
      "Epoch: 33/300..  Training Loss: 0.321..  Test Loss: 0.362.. \n",
      "Epoch: 34/300..  Training Loss: 0.318..  Test Loss: 0.358.. \n",
      "Epoch: 35/300..  Training Loss: 0.314..  Test Loss: 0.354.. \n",
      "Epoch: 36/300..  Training Loss: 0.310..  Test Loss: 0.351.. \n",
      "Epoch: 37/300..  Training Loss: 0.307..  Test Loss: 0.347.. \n",
      "Epoch: 38/300..  Training Loss: 0.303..  Test Loss: 0.343.. \n",
      "Epoch: 39/300..  Training Loss: 0.299..  Test Loss: 0.339.. \n",
      "Epoch: 40/300..  Training Loss: 0.295..  Test Loss: 0.335.. \n",
      "Epoch: 41/300..  Training Loss: 0.291..  Test Loss: 0.331.. \n",
      "Epoch: 42/300..  Training Loss: 0.287..  Test Loss: 0.326.. \n",
      "Epoch: 43/300..  Training Loss: 0.283..  Test Loss: 0.322.. \n",
      "Epoch: 44/300..  Training Loss: 0.279..  Test Loss: 0.318.. \n",
      "Epoch: 45/300..  Training Loss: 0.275..  Test Loss: 0.313.. \n",
      "Epoch: 46/300..  Training Loss: 0.271..  Test Loss: 0.309.. \n",
      "Epoch: 47/300..  Training Loss: 0.267..  Test Loss: 0.304.. \n",
      "Epoch: 48/300..  Training Loss: 0.263..  Test Loss: 0.300.. \n",
      "Epoch: 49/300..  Training Loss: 0.259..  Test Loss: 0.296.. \n",
      "Epoch: 50/300..  Training Loss: 0.255..  Test Loss: 0.291.. \n",
      "Epoch: 51/300..  Training Loss: 0.251..  Test Loss: 0.287.. \n",
      "Epoch: 52/300..  Training Loss: 0.247..  Test Loss: 0.283.. \n",
      "Epoch: 53/300..  Training Loss: 0.243..  Test Loss: 0.278.. \n",
      "Epoch: 54/300..  Training Loss: 0.239..  Test Loss: 0.274.. \n",
      "Epoch: 55/300..  Training Loss: 0.235..  Test Loss: 0.270.. \n",
      "Epoch: 56/300..  Training Loss: 0.232..  Test Loss: 0.266.. \n",
      "Epoch: 57/300..  Training Loss: 0.228..  Test Loss: 0.262.. \n",
      "Epoch: 58/300..  Training Loss: 0.225..  Test Loss: 0.259.. \n",
      "Epoch: 59/300..  Training Loss: 0.222..  Test Loss: 0.255.. \n",
      "Epoch: 60/300..  Training Loss: 0.219..  Test Loss: 0.252.. \n",
      "Epoch: 61/300..  Training Loss: 0.216..  Test Loss: 0.249.. \n",
      "Epoch: 62/300..  Training Loss: 0.214..  Test Loss: 0.246.. \n",
      "Epoch: 63/300..  Training Loss: 0.211..  Test Loss: 0.243.. \n",
      "Epoch: 64/300..  Training Loss: 0.209..  Test Loss: 0.240.. \n",
      "Epoch: 65/300..  Training Loss: 0.206..  Test Loss: 0.238.. \n",
      "Epoch: 66/300..  Training Loss: 0.204..  Test Loss: 0.235.. \n",
      "Epoch: 67/300..  Training Loss: 0.202..  Test Loss: 0.233.. \n",
      "Epoch: 68/300..  Training Loss: 0.200..  Test Loss: 0.231.. \n",
      "Epoch: 69/300..  Training Loss: 0.198..  Test Loss: 0.229.. \n",
      "Epoch: 70/300..  Training Loss: 0.196..  Test Loss: 0.227.. \n",
      "Epoch: 71/300..  Training Loss: 0.195..  Test Loss: 0.225.. \n",
      "Epoch: 72/300..  Training Loss: 0.193..  Test Loss: 0.224.. \n",
      "Epoch: 73/300..  Training Loss: 0.191..  Test Loss: 0.222.. \n",
      "Epoch: 74/300..  Training Loss: 0.189..  Test Loss: 0.220.. \n",
      "Epoch: 75/300..  Training Loss: 0.188..  Test Loss: 0.219.. \n",
      "Epoch: 76/300..  Training Loss: 0.186..  Test Loss: 0.217.. \n",
      "Epoch: 77/300..  Training Loss: 0.185..  Test Loss: 0.216.. \n",
      "Epoch: 78/300..  Training Loss: 0.183..  Test Loss: 0.214.. \n",
      "Epoch: 79/300..  Training Loss: 0.181..  Test Loss: 0.213.. \n",
      "Epoch: 80/300..  Training Loss: 0.180..  Test Loss: 0.212.. \n",
      "Epoch: 81/300..  Training Loss: 0.178..  Test Loss: 0.210.. \n",
      "Epoch: 82/300..  Training Loss: 0.177..  Test Loss: 0.209.. \n",
      "Epoch: 83/300..  Training Loss: 0.175..  Test Loss: 0.208.. \n",
      "Epoch: 84/300..  Training Loss: 0.174..  Test Loss: 0.207.. \n",
      "Epoch: 85/300..  Training Loss: 0.173..  Test Loss: 0.205.. \n",
      "Epoch: 86/300..  Training Loss: 0.171..  Test Loss: 0.204.. \n",
      "Epoch: 87/300..  Training Loss: 0.170..  Test Loss: 0.203.. \n",
      "Epoch: 88/300..  Training Loss: 0.168..  Test Loss: 0.202.. \n",
      "Epoch: 89/300..  Training Loss: 0.167..  Test Loss: 0.201.. \n",
      "Epoch: 90/300..  Training Loss: 0.166..  Test Loss: 0.200.. \n",
      "Epoch: 91/300..  Training Loss: 0.164..  Test Loss: 0.199.. \n",
      "Epoch: 92/300..  Training Loss: 0.163..  Test Loss: 0.198.. \n",
      "Epoch: 93/300..  Training Loss: 0.162..  Test Loss: 0.197.. \n",
      "Epoch: 94/300..  Training Loss: 0.160..  Test Loss: 0.196.. \n",
      "Epoch: 95/300..  Training Loss: 0.159..  Test Loss: 0.195.. \n",
      "Epoch: 96/300..  Training Loss: 0.158..  Test Loss: 0.194.. \n",
      "Epoch: 97/300..  Training Loss: 0.157..  Test Loss: 0.194.. \n",
      "Epoch: 98/300..  Training Loss: 0.156..  Test Loss: 0.193.. \n",
      "Epoch: 99/300..  Training Loss: 0.154..  Test Loss: 0.192.. \n",
      "Epoch: 100/300..  Training Loss: 0.153..  Test Loss: 0.191.. \n",
      "Epoch: 101/300..  Training Loss: 0.152..  Test Loss: 0.190.. \n",
      "Epoch: 102/300..  Training Loss: 0.151..  Test Loss: 0.190.. \n",
      "Epoch: 103/300..  Training Loss: 0.150..  Test Loss: 0.189.. \n",
      "Epoch: 104/300..  Training Loss: 0.149..  Test Loss: 0.188.. \n",
      "Epoch: 105/300..  Training Loss: 0.148..  Test Loss: 0.188.. \n",
      "Epoch: 106/300..  Training Loss: 0.147..  Test Loss: 0.187.. \n",
      "Epoch: 107/300..  Training Loss: 0.146..  Test Loss: 0.186.. \n",
      "Epoch: 108/300..  Training Loss: 0.145..  Test Loss: 0.186.. \n",
      "Epoch: 109/300..  Training Loss: 0.144..  Test Loss: 0.185.. \n",
      "Epoch: 110/300..  Training Loss: 0.143..  Test Loss: 0.184.. \n",
      "Epoch: 111/300..  Training Loss: 0.142..  Test Loss: 0.184.. \n",
      "Epoch: 112/300..  Training Loss: 0.141..  Test Loss: 0.183.. \n",
      "Epoch: 113/300..  Training Loss: 0.140..  Test Loss: 0.183.. \n",
      "Epoch: 114/300..  Training Loss: 0.139..  Test Loss: 0.182.. \n",
      "Epoch: 115/300..  Training Loss: 0.139..  Test Loss: 0.182.. \n",
      "Epoch: 116/300..  Training Loss: 0.138..  Test Loss: 0.181.. \n",
      "Epoch: 117/300..  Training Loss: 0.137..  Test Loss: 0.181.. \n",
      "Epoch: 118/300..  Training Loss: 0.136..  Test Loss: 0.180.. \n",
      "Epoch: 119/300..  Training Loss: 0.135..  Test Loss: 0.180.. \n",
      "Epoch: 120/300..  Training Loss: 0.134..  Test Loss: 0.179.. \n",
      "Epoch: 121/300..  Training Loss: 0.134..  Test Loss: 0.179.. \n",
      "Epoch: 122/300..  Training Loss: 0.133..  Test Loss: 0.178.. \n",
      "Epoch: 123/300..  Training Loss: 0.132..  Test Loss: 0.178.. \n",
      "Epoch: 124/300..  Training Loss: 0.131..  Test Loss: 0.177.. \n",
      "Epoch: 125/300..  Training Loss: 0.131..  Test Loss: 0.177.. \n",
      "Epoch: 126/300..  Training Loss: 0.130..  Test Loss: 0.176.. \n",
      "Epoch: 127/300..  Training Loss: 0.129..  Test Loss: 0.176.. \n",
      "Epoch: 128/300..  Training Loss: 0.128..  Test Loss: 0.175.. \n",
      "Epoch: 129/300..  Training Loss: 0.128..  Test Loss: 0.175.. \n",
      "Epoch: 130/300..  Training Loss: 0.127..  Test Loss: 0.174.. \n",
      "Epoch: 131/300..  Training Loss: 0.126..  Test Loss: 0.174.. \n",
      "Epoch: 132/300..  Training Loss: 0.126..  Test Loss: 0.174.. \n",
      "Epoch: 133/300..  Training Loss: 0.125..  Test Loss: 0.173.. \n",
      "Epoch: 134/300..  Training Loss: 0.124..  Test Loss: 0.173.. \n",
      "Epoch: 135/300..  Training Loss: 0.124..  Test Loss: 0.172.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/300..  Training Loss: 0.123..  Test Loss: 0.172.. \n",
      "Epoch: 137/300..  Training Loss: 0.122..  Test Loss: 0.171.. \n",
      "Epoch: 138/300..  Training Loss: 0.122..  Test Loss: 0.171.. \n",
      "Epoch: 139/300..  Training Loss: 0.121..  Test Loss: 0.170.. \n",
      "Epoch: 140/300..  Training Loss: 0.121..  Test Loss: 0.170.. \n",
      "Epoch: 141/300..  Training Loss: 0.120..  Test Loss: 0.170.. \n",
      "Epoch: 142/300..  Training Loss: 0.119..  Test Loss: 0.169.. \n",
      "Epoch: 143/300..  Training Loss: 0.119..  Test Loss: 0.169.. \n",
      "Epoch: 144/300..  Training Loss: 0.118..  Test Loss: 0.168.. \n",
      "Epoch: 145/300..  Training Loss: 0.118..  Test Loss: 0.168.. \n",
      "Epoch: 146/300..  Training Loss: 0.117..  Test Loss: 0.168.. \n",
      "Epoch: 147/300..  Training Loss: 0.117..  Test Loss: 0.167.. \n",
      "Epoch: 148/300..  Training Loss: 0.116..  Test Loss: 0.167.. \n",
      "Epoch: 149/300..  Training Loss: 0.116..  Test Loss: 0.166.. \n",
      "Epoch: 150/300..  Training Loss: 0.115..  Test Loss: 0.166.. \n",
      "Epoch: 151/300..  Training Loss: 0.115..  Test Loss: 0.166.. \n",
      "Epoch: 152/300..  Training Loss: 0.114..  Test Loss: 0.165.. \n",
      "Epoch: 153/300..  Training Loss: 0.114..  Test Loss: 0.165.. \n",
      "Epoch: 154/300..  Training Loss: 0.113..  Test Loss: 0.165.. \n",
      "Epoch: 155/300..  Training Loss: 0.113..  Test Loss: 0.164.. \n",
      "Epoch: 156/300..  Training Loss: 0.112..  Test Loss: 0.164.. \n",
      "Epoch: 157/300..  Training Loss: 0.112..  Test Loss: 0.164.. \n",
      "Epoch: 158/300..  Training Loss: 0.111..  Test Loss: 0.163.. \n",
      "Epoch: 159/300..  Training Loss: 0.111..  Test Loss: 0.163.. \n",
      "Epoch: 160/300..  Training Loss: 0.110..  Test Loss: 0.163.. \n",
      "Epoch: 161/300..  Training Loss: 0.110..  Test Loss: 0.162.. \n",
      "Epoch: 162/300..  Training Loss: 0.109..  Test Loss: 0.162.. \n",
      "Epoch: 163/300..  Training Loss: 0.109..  Test Loss: 0.162.. \n",
      "Epoch: 164/300..  Training Loss: 0.108..  Test Loss: 0.161.. \n",
      "Epoch: 165/300..  Training Loss: 0.108..  Test Loss: 0.161.. \n",
      "Epoch: 166/300..  Training Loss: 0.108..  Test Loss: 0.161.. \n",
      "Epoch: 167/300..  Training Loss: 0.107..  Test Loss: 0.160.. \n",
      "Epoch: 168/300..  Training Loss: 0.107..  Test Loss: 0.160.. \n",
      "Epoch: 169/300..  Training Loss: 0.106..  Test Loss: 0.160.. \n",
      "Epoch: 170/300..  Training Loss: 0.106..  Test Loss: 0.160.. \n",
      "Epoch: 171/300..  Training Loss: 0.106..  Test Loss: 0.159.. \n",
      "Epoch: 172/300..  Training Loss: 0.105..  Test Loss: 0.159.. \n",
      "Epoch: 173/300..  Training Loss: 0.105..  Test Loss: 0.159.. \n",
      "Epoch: 174/300..  Training Loss: 0.104..  Test Loss: 0.159.. \n",
      "Epoch: 175/300..  Training Loss: 0.104..  Test Loss: 0.158.. \n",
      "Epoch: 176/300..  Training Loss: 0.104..  Test Loss: 0.158.. \n",
      "Epoch: 177/300..  Training Loss: 0.103..  Test Loss: 0.158.. \n",
      "Epoch: 178/300..  Training Loss: 0.103..  Test Loss: 0.158.. \n",
      "Epoch: 179/300..  Training Loss: 0.102..  Test Loss: 0.157.. \n",
      "Epoch: 180/300..  Training Loss: 0.102..  Test Loss: 0.157.. \n",
      "Epoch: 181/300..  Training Loss: 0.102..  Test Loss: 0.157.. \n",
      "Epoch: 182/300..  Training Loss: 0.101..  Test Loss: 0.157.. \n",
      "Epoch: 183/300..  Training Loss: 0.101..  Test Loss: 0.157.. \n",
      "Epoch: 184/300..  Training Loss: 0.101..  Test Loss: 0.156.. \n",
      "Epoch: 185/300..  Training Loss: 0.100..  Test Loss: 0.156.. \n",
      "Epoch: 186/300..  Training Loss: 0.100..  Test Loss: 0.156.. \n",
      "Epoch: 187/300..  Training Loss: 0.100..  Test Loss: 0.156.. \n",
      "Epoch: 188/300..  Training Loss: 0.099..  Test Loss: 0.156.. \n",
      "Epoch: 189/300..  Training Loss: 0.099..  Test Loss: 0.155.. \n",
      "Epoch: 190/300..  Training Loss: 0.099..  Test Loss: 0.155.. \n",
      "Epoch: 191/300..  Training Loss: 0.098..  Test Loss: 0.155.. \n",
      "Epoch: 192/300..  Training Loss: 0.098..  Test Loss: 0.155.. \n",
      "Epoch: 193/300..  Training Loss: 0.098..  Test Loss: 0.155.. \n",
      "Epoch: 194/300..  Training Loss: 0.097..  Test Loss: 0.155.. \n",
      "Epoch: 195/300..  Training Loss: 0.097..  Test Loss: 0.155.. \n",
      "Epoch: 196/300..  Training Loss: 0.097..  Test Loss: 0.154.. \n",
      "Epoch: 197/300..  Training Loss: 0.096..  Test Loss: 0.154.. \n",
      "Epoch: 198/300..  Training Loss: 0.096..  Test Loss: 0.154.. \n",
      "Epoch: 199/300..  Training Loss: 0.096..  Test Loss: 0.154.. \n",
      "Epoch: 200/300..  Training Loss: 0.095..  Test Loss: 0.154.. \n",
      "Epoch: 201/300..  Training Loss: 0.095..  Test Loss: 0.154.. \n",
      "Epoch: 202/300..  Training Loss: 0.095..  Test Loss: 0.154.. \n",
      "Epoch: 203/300..  Training Loss: 0.095..  Test Loss: 0.153.. \n",
      "Epoch: 204/300..  Training Loss: 0.094..  Test Loss: 0.153.. \n",
      "Epoch: 205/300..  Training Loss: 0.094..  Test Loss: 0.153.. \n",
      "Epoch: 206/300..  Training Loss: 0.094..  Test Loss: 0.153.. \n",
      "Epoch: 207/300..  Training Loss: 0.093..  Test Loss: 0.153.. \n",
      "Epoch: 208/300..  Training Loss: 0.093..  Test Loss: 0.153.. \n",
      "Epoch: 209/300..  Training Loss: 0.093..  Test Loss: 0.153.. \n",
      "Epoch: 210/300..  Training Loss: 0.093..  Test Loss: 0.153.. \n",
      "Epoch: 211/300..  Training Loss: 0.092..  Test Loss: 0.153.. \n",
      "Epoch: 212/300..  Training Loss: 0.092..  Test Loss: 0.153.. \n",
      "Epoch: 213/300..  Training Loss: 0.092..  Test Loss: 0.153.. \n",
      "Epoch: 214/300..  Training Loss: 0.092..  Test Loss: 0.152.. \n",
      "Epoch: 215/300..  Training Loss: 0.091..  Test Loss: 0.152.. \n",
      "Epoch: 216/300..  Training Loss: 0.091..  Test Loss: 0.152.. \n",
      "Epoch: 217/300..  Training Loss: 0.091..  Test Loss: 0.152.. \n",
      "Epoch: 218/300..  Training Loss: 0.090..  Test Loss: 0.152.. \n",
      "Epoch: 219/300..  Training Loss: 0.090..  Test Loss: 0.152.. \n",
      "Epoch: 220/300..  Training Loss: 0.090..  Test Loss: 0.152.. \n",
      "Epoch: 221/300..  Training Loss: 0.090..  Test Loss: 0.152.. \n",
      "Epoch: 222/300..  Training Loss: 0.089..  Test Loss: 0.152.. \n",
      "Epoch: 223/300..  Training Loss: 0.089..  Test Loss: 0.152.. \n",
      "Epoch: 224/300..  Training Loss: 0.089..  Test Loss: 0.152.. \n",
      "Epoch: 225/300..  Training Loss: 0.089..  Test Loss: 0.152.. \n",
      "Epoch: 226/300..  Training Loss: 0.088..  Test Loss: 0.152.. \n",
      "Epoch: 227/300..  Training Loss: 0.088..  Test Loss: 0.152.. \n",
      "Epoch: 228/300..  Training Loss: 0.088..  Test Loss: 0.152.. \n",
      "Epoch: 229/300..  Training Loss: 0.088..  Test Loss: 0.152.. \n",
      "Epoch: 230/300..  Training Loss: 0.088..  Test Loss: 0.152.. \n",
      "Epoch: 231/300..  Training Loss: 0.087..  Test Loss: 0.151.. \n",
      "Epoch: 232/300..  Training Loss: 0.087..  Test Loss: 0.151.. \n",
      "Epoch: 233/300..  Training Loss: 0.087..  Test Loss: 0.151.. \n",
      "Epoch: 234/300..  Training Loss: 0.087..  Test Loss: 0.151.. \n",
      "Epoch: 235/300..  Training Loss: 0.086..  Test Loss: 0.151.. \n",
      "Epoch: 236/300..  Training Loss: 0.086..  Test Loss: 0.151.. \n",
      "Epoch: 237/300..  Training Loss: 0.086..  Test Loss: 0.151.. \n",
      "Epoch: 238/300..  Training Loss: 0.086..  Test Loss: 0.151.. \n",
      "Epoch: 239/300..  Training Loss: 0.086..  Test Loss: 0.151.. \n",
      "Epoch: 240/300..  Training Loss: 0.085..  Test Loss: 0.151.. \n",
      "Epoch: 241/300..  Training Loss: 0.085..  Test Loss: 0.151.. \n",
      "Epoch: 242/300..  Training Loss: 0.085..  Test Loss: 0.151.. \n",
      "Epoch: 243/300..  Training Loss: 0.085..  Test Loss: 0.151.. \n",
      "Epoch: 244/300..  Training Loss: 0.084..  Test Loss: 0.151.. \n",
      "Epoch: 245/300..  Training Loss: 0.084..  Test Loss: 0.151.. \n",
      "Epoch: 246/300..  Training Loss: 0.084..  Test Loss: 0.151.. \n",
      "Epoch: 247/300..  Training Loss: 0.084..  Test Loss: 0.151.. \n",
      "Epoch: 248/300..  Training Loss: 0.084..  Test Loss: 0.151.. \n",
      "Epoch: 249/300..  Training Loss: 0.083..  Test Loss: 0.151.. \n",
      "Epoch: 250/300..  Training Loss: 0.083..  Test Loss: 0.151.. \n",
      "Epoch: 251/300..  Training Loss: 0.083..  Test Loss: 0.151.. \n",
      "Epoch: 252/300..  Training Loss: 0.083..  Test Loss: 0.151.. \n",
      "Epoch: 253/300..  Training Loss: 0.083..  Test Loss: 0.151.. \n",
      "Epoch: 254/300..  Training Loss: 0.082..  Test Loss: 0.151.. \n",
      "Epoch: 255/300..  Training Loss: 0.082..  Test Loss: 0.151.. \n",
      "Epoch: 256/300..  Training Loss: 0.082..  Test Loss: 0.151.. \n",
      "Epoch: 257/300..  Training Loss: 0.082..  Test Loss: 0.151.. \n",
      "Epoch: 258/300..  Training Loss: 0.082..  Test Loss: 0.151.. \n",
      "Epoch: 259/300..  Training Loss: 0.081..  Test Loss: 0.151.. \n",
      "Epoch: 260/300..  Training Loss: 0.081..  Test Loss: 0.151.. \n",
      "Epoch: 261/300..  Training Loss: 0.081..  Test Loss: 0.151.. \n",
      "Epoch: 262/300..  Training Loss: 0.081..  Test Loss: 0.151.. \n",
      "Epoch: 263/300..  Training Loss: 0.081..  Test Loss: 0.151.. \n",
      "Epoch: 264/300..  Training Loss: 0.081..  Test Loss: 0.151.. \n",
      "Epoch: 265/300..  Training Loss: 0.080..  Test Loss: 0.151.. \n",
      "Epoch: 266/300..  Training Loss: 0.080..  Test Loss: 0.151.. \n",
      "Epoch: 267/300..  Training Loss: 0.080..  Test Loss: 0.151.. \n",
      "Epoch: 268/300..  Training Loss: 0.080..  Test Loss: 0.151.. \n",
      "Epoch: 269/300..  Training Loss: 0.080..  Test Loss: 0.151.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 270/300..  Training Loss: 0.080..  Test Loss: 0.151.. \n",
      "Epoch: 271/300..  Training Loss: 0.079..  Test Loss: 0.151.. \n",
      "Epoch: 272/300..  Training Loss: 0.079..  Test Loss: 0.151.. \n",
      "Epoch: 273/300..  Training Loss: 0.079..  Test Loss: 0.151.. \n",
      "Epoch: 274/300..  Training Loss: 0.079..  Test Loss: 0.151.. \n",
      "Epoch: 275/300..  Training Loss: 0.079..  Test Loss: 0.151.. \n",
      "Epoch: 276/300..  Training Loss: 0.078..  Test Loss: 0.151.. \n",
      "Epoch: 277/300..  Training Loss: 0.078..  Test Loss: 0.151.. \n",
      "Epoch: 278/300..  Training Loss: 0.078..  Test Loss: 0.151.. \n",
      "Epoch: 279/300..  Training Loss: 0.078..  Test Loss: 0.151.. \n",
      "Epoch: 280/300..  Training Loss: 0.078..  Test Loss: 0.151.. \n",
      "Epoch: 281/300..  Training Loss: 0.078..  Test Loss: 0.151.. \n",
      "Epoch: 282/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 283/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 284/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 285/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 286/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 287/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 288/300..  Training Loss: 0.077..  Test Loss: 0.151.. \n",
      "Epoch: 289/300..  Training Loss: 0.076..  Test Loss: 0.151.. \n",
      "Epoch: 290/300..  Training Loss: 0.076..  Test Loss: 0.151.. \n",
      "Epoch: 291/300..  Training Loss: 0.076..  Test Loss: 0.151.. \n",
      "Epoch: 292/300..  Training Loss: 0.076..  Test Loss: 0.151.. \n",
      "Epoch: 293/300..  Training Loss: 0.076..  Test Loss: 0.151.. \n",
      "Epoch: 294/300..  Training Loss: 0.076..  Test Loss: 0.151.. \n",
      "Epoch: 295/300..  Training Loss: 0.075..  Test Loss: 0.151.. \n",
      "Epoch: 296/300..  Training Loss: 0.075..  Test Loss: 0.151.. \n",
      "Epoch: 297/300..  Training Loss: 0.075..  Test Loss: 0.151.. \n",
      "Epoch: 298/300..  Training Loss: 0.075..  Test Loss: 0.151.. \n",
      "Epoch: 299/300..  Training Loss: 0.075..  Test Loss: 0.151.. \n",
      "Epoch: 300/300..  Training Loss: 0.075..  Test Loss: 0.151.. \n"
     ]
    }
   ],
   "source": [
    "model = Regressor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "42397ea642981a38da489ccf809cb1b1a439a82d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7feb2518e550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "9e47c4c950405b73ed40ccfad72b1446686156b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.from_numpy(test.values).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.forward(test)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128283.6875],\n",
      "        [157806.5938],\n",
      "        [178393.7500],\n",
      "        ...,\n",
      "        [163115.5156],\n",
      "        [112040.8750],\n",
      "        [203125.7656]])\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "bf0ac560c4a12c9e72afc1662db2c0b62b43c3cd"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['SalePrice'] = output.numpy()\n",
    "submission.to_csv('Regression_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8627a9dda9d660918bc53453e348c02fae3ed6dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>128283.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>157806.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>178393.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>183896.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>192994.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1466</td>\n",
       "      <td>177651.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1467</td>\n",
       "      <td>170787.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1468</td>\n",
       "      <td>161059.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1469</td>\n",
       "      <td>176284.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1470</td>\n",
       "      <td>133028.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1471</td>\n",
       "      <td>167342.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1472</td>\n",
       "      <td>102880.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1473</td>\n",
       "      <td>103407.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1474</td>\n",
       "      <td>140889.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1475</td>\n",
       "      <td>108961.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1476</td>\n",
       "      <td>357233.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1477</td>\n",
       "      <td>245291.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1478</td>\n",
       "      <td>309247.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1479</td>\n",
       "      <td>314763.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1480</td>\n",
       "      <td>473454.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1481</td>\n",
       "      <td>320853.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1482</td>\n",
       "      <td>206637.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1483</td>\n",
       "      <td>170993.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1484</td>\n",
       "      <td>162612.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1485</td>\n",
       "      <td>178380.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1486</td>\n",
       "      <td>182926.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1487</td>\n",
       "      <td>339540.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1488</td>\n",
       "      <td>225459.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1489</td>\n",
       "      <td>195181.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1490</td>\n",
       "      <td>241239.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2890</td>\n",
       "      <td>87329.554688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2891</td>\n",
       "      <td>141748.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2892</td>\n",
       "      <td>34770.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2893</td>\n",
       "      <td>77055.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2894</td>\n",
       "      <td>43797.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2895</td>\n",
       "      <td>351456.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2896</td>\n",
       "      <td>288933.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2897</td>\n",
       "      <td>191508.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2898</td>\n",
       "      <td>145286.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2899</td>\n",
       "      <td>201854.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2900</td>\n",
       "      <td>164549.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2901</td>\n",
       "      <td>249278.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2902</td>\n",
       "      <td>180544.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2903</td>\n",
       "      <td>309944.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2904</td>\n",
       "      <td>345441.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2905</td>\n",
       "      <td>65226.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2906</td>\n",
       "      <td>170255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2907</td>\n",
       "      <td>108700.382812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2908</td>\n",
       "      <td>136934.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>2909</td>\n",
       "      <td>145529.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>2910</td>\n",
       "      <td>74096.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>2911</td>\n",
       "      <td>80563.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2912</td>\n",
       "      <td>151084.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>2913</td>\n",
       "      <td>76104.101562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>2914</td>\n",
       "      <td>78821.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>88958.757812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>75299.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>163115.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>112040.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>203125.765625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  128283.687500\n",
       "1     1462  157806.593750\n",
       "2     1463  178393.750000\n",
       "3     1464  183896.296875\n",
       "4     1465  192994.140625\n",
       "5     1466  177651.031250\n",
       "6     1467  170787.812500\n",
       "7     1468  161059.968750\n",
       "8     1469  176284.171875\n",
       "9     1470  133028.343750\n",
       "10    1471  167342.562500\n",
       "11    1472  102880.023438\n",
       "12    1473  103407.929688\n",
       "13    1474  140889.890625\n",
       "14    1475  108961.609375\n",
       "15    1476  357233.937500\n",
       "16    1477  245291.890625\n",
       "17    1478  309247.843750\n",
       "18    1479  314763.531250\n",
       "19    1480  473454.843750\n",
       "20    1481  320853.312500\n",
       "21    1482  206637.531250\n",
       "22    1483  170993.359375\n",
       "23    1484  162612.109375\n",
       "24    1485  178380.656250\n",
       "25    1486  182926.234375\n",
       "26    1487  339540.750000\n",
       "27    1488  225459.906250\n",
       "28    1489  195181.781250\n",
       "29    1490  241239.968750\n",
       "...    ...            ...\n",
       "1429  2890   87329.554688\n",
       "1430  2891  141748.218750\n",
       "1431  2892   34770.562500\n",
       "1432  2893   77055.375000\n",
       "1433  2894   43797.953125\n",
       "1434  2895  351456.312500\n",
       "1435  2896  288933.937500\n",
       "1436  2897  191508.734375\n",
       "1437  2898  145286.406250\n",
       "1438  2899  201854.718750\n",
       "1439  2900  164549.781250\n",
       "1440  2901  249278.343750\n",
       "1441  2902  180544.171875\n",
       "1442  2903  309944.187500\n",
       "1443  2904  345441.906250\n",
       "1444  2905   65226.812500\n",
       "1445  2906  170255.000000\n",
       "1446  2907  108700.382812\n",
       "1447  2908  136934.578125\n",
       "1448  2909  145529.906250\n",
       "1449  2910   74096.546875\n",
       "1450  2911   80563.617188\n",
       "1451  2912  151084.375000\n",
       "1452  2913   76104.101562\n",
       "1453  2914   78821.898438\n",
       "1454  2915   88958.757812\n",
       "1455  2916   75299.937500\n",
       "1456  2917  163115.515625\n",
       "1457  2918  112040.875000\n",
       "1458  2919  203125.765625\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
